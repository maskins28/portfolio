---
title: "Cloud Prediction Lab"
author: "Michael Askins"
format: 
  html:
    #code-fold: true
    #code-summary: "Show Code"
    code-tools: true
    theme: sandstone
    lightbox: true
    embed-resources: true
execute: 
  warning: false
  message: false
---

```{r setup}
# source all functions from R/ folder
for (fname in list.files(here::here("R"), pattern = "*.R")) {
  source(here::here(file.path("R", fname)))
}

# path to data
DATA_PATH <- here::here("data")

# set seed
set.seed(331)

# load in required libraries
library(patchwork)
library(nnet)
library(caret)
library(glmnet)
library(randomForest)
library(DT)
library(tidyverse)
```

```{r loading data, message=FALSE}
# loading in data, custom function from load.R

#read_image <- function(path, n) {
#  full_path <- paste(path,'/image',n,'.txt',sep = '')
#  df <- read_table(full_path, col_names = F)
#  colnames(df) <- c('y','x','label','NDAI','SD','CORR',
#                    'DF','CF','BF','AF','AN')
#  df <- mutate(df, label = factor(label))
#  return(df)
#}

image1 <- read_image(DATA_PATH, 1)
image2 <- read_image(DATA_PATH, 2)
image3 <- read_image(DATA_PATH, 3)
```

# Exploratory Data Analysis

We aim to be able to predict the presence of clouds in snowy areas through the use of various radiance observations and engineered features. Looking first at the radiances, we see that all five are quite highly correlated, with AF and AN being somewhat less correlated with DF. They all appear to be approximately linearly related to each other, with differing variances depending on the combinations. All five also have a very similarly shaped, bimodal left-skewed distribution. Looking next at the engineered features, we see less correlation among them, although still significant. The shapes of their relationships appear non-linear, and they have differently shaped distributions. Finally, looking briefly at how they are related to each other and to the labels, we largely see small, negative but significant correlations between the radiances and the engineered features.

```{r plotting cloud labels, fig.height=3, fig.cap='Figure 1: Images of Cloud Labels'}
# Plotting cloud maps, custom function from plotting.R


#plot_cloud_image <- function(image, legend = F) {
#  ggplot(image, aes(x = x, y = y, fill = label)) +
#    geom_raster() +
#    scale_fill_manual(limits = c('-1','0','1'),
#                      labels = c('Cloud','Uncertain','No Cloud'),
#                      values = c('lightgrey','grey','darkgrey')) +
#    theme_void() +
#    theme(legend.position = ifelse(legend,'right','none'),
#          text = element_text(size = 12, family = 'serif')) +
#    coord_fixed() +
#    labs(fill = '')
#}


p1 <- plot_cloud_image(image1)
p2 <- plot_cloud_image(image2)
p3 <- plot_cloud_image(image3, legend = T)
p1+p2+p3
```

```{r plotting feature relationships, message = F}
# Constructing a full dataset by merging the data from the three images
full_data <- rbind(image1,image2,image3) 

# Plotting the relationships between the radiance features 
# For the following plots, only a sample of the data is plotted for computational efficiency 
ggpairs(slice_sample(full_data, n = 10000), columns = c(7:11))

# Plotting the relationships between the engineered features
ggpairs(slice_sample(full_data, n = 10000), columns = c(4:6))

# Plotting the relationships between all features and class labels
ggpairs(slice_sample(full_data, n = 10000), columns = c(3:11))
```

Looking at the relationships between the expert labels and each feature certain trends do emerge. The most obvious one is that the NDAI engineered feature has a stark difference between the no clouds label and the cloud vs uncertain label. The other two engineered features show a general relationship of having a higher mean value for the cloud label as opposed to the no cloud label, with the uncertain label falling between them. As for the radiances, the opposite relationship emerges with the cloud label having lower values than the no cloud label, with the exception of the DF label which shows very little variance among the labels.

```{r plotting label relationships}
# Reformatting the data for plotting
full_data %>%
  select(3:11) %>%
  pivot_longer(cols = -label,
               values_to = 'values', names_to = 'features') %>%

# Plotting boxplots of each feature and class label combination to find trends
ggplot(aes(x = label, y = values)) +
  geom_boxplot() +
  facet_wrap(~ features, scales = 'free_y', nrow = 2)
```

# Prediction Modeling

### Data Splitting

We first perform data splitting on the given data. While the optimal strategy of data splitting in this case would be to use some images for training, some for validation, and some for testing, as new data would be received in this manner, the fact we only have three images makes this difficult as we can't get the optimal split. So instead we combine the data from all three images and then randomly split the data up into a 60% training, 15% validation, and 25% training partiton.

```{r data splitting}
# Splitting full data between train and test
sample_size <- floor(0.75 * nrow(full_data))
train_indices <- sample(seq_len(nrow(full_data)), size = sample_size)

train_data <- full_data[train_indices, ]
test_data <- full_data[-train_indices, ]

# Splitting train data between train and validation
sample_size <- floor(0.8 * nrow(train_data))
train_indices <- sample(seq_len(nrow(train_data)), size = sample_size)

valid_data <- train_data[-train_indices, ]
train_data <- train_data[train_indices, ]
```

### Data Preprocessing

We have two pathways for modeling the data to handle the "uncertain" expert labels. One being to turn it into a binomial classification problem, either removing the uncertain labels or placing them within another group, or keeping all three labels and turning it into a multinomial classification problem, having our models predicting these "uncertain" labels. Both have their merits and certain modeling processes only work with binomial data, so we will proceed with two models for each pathway. To do this we relabel all uncertain data to no clouds, choosing the presence of clouds as the label we want to be more certain of. This is a subjective decision and the correct one may vary from use case to use case. One pathway, ridge regression, also requires scaled data so we create that dataset as well. In summary, two models (multinomial logistic regression and random forests) will use the data as is, while binomial logistic regression will use the binary data and ridge regression the scaled binary dataset.

```{r data preprocessing}
# Regression formulas
engineered_form <- 'label ~ NDAI + SD + CORR'
radiance_form <- 'label ~ DF + CF + BF + AF + AN'
full_form <- 'label ~ NDAI + SD + CORR + DF + CF + BF + AF + AN'

# Transforming (-1, 0, 1) data into binary (0, 1) data
# Classifying uncertain (0) as no clouds

train_data_binary <- train_data %>%
  mutate(label = case_when(label == -1 ~ 0,
                           label == 0 ~ 0,
                           T ~ 1)) 

valid_data_binary <- valid_data %>%
  mutate(label = case_when(label == -1 ~ 0,
                           label == 0 ~ 0,
                           T ~ 1)) 

# Scaling binary data for models that require it

pre_proc_val <- preProcess(train_data_binary[,c(4:11)], 
                           method = c("center", "scale"))
train_data_binary_scaled <- cbind(train_data_binary[,c(1:3)], 
                     predict(pre_proc_val, train_data_binary[,c(4:11)]))

pre_proc_val <- preProcess(valid_data_binary[,c(4:11)], 
                           method = c("center", "scale"))
valid_data_binary_scaled <- cbind(valid_data_binary[,c(1:3)], 
                     predict(pre_proc_val, valid_data[,c(4:11)]))
```

### Binomial Logistic Regression

The first classification model used is traditional binary logistic regression. This essentially uses linear regression principles to yield the predicted probabilities of a "success" or "failure," in this case cloud or no cloud, which is why it requires binary data. The predicted probabilities are turned into labels by labeling an observation cloud if the probability is above 0.5, and not cloud otherwise. The model assumes a linear relationship between the predictors and the log-odds of a success or failure, which is difficult to assess if with the range of predictors but is likely reasonable given the linearity of the no cloud-uncertain-cloud means for each feature visualized previously. The models are run here and the accuracy rate (proportion of correct labels) for the validation dataset is recorded and collected later for all models.

```{r binomial logistic}
# Training binomial logistic regression models
bin_eng_mod <- glm(engineered_form, 
                   data = train_data_binary, family = 'binomial')
bin_rad_mod <- glm(radiance_form, 
                   data = train_data_binary, family = 'binomial')
bin_full_mod <- glm(full_form, 
            data = train_data_binary, family = 'binomial')

# Getting predictions and accuracies for binomial regression
bin_eng_preds <- case_when(predict(bin_eng_mod, 
                                valid_data, 
                                type = 'response') > 0.5 ~ 1, T ~ 0)
bin_eng_acc <- mean(bin_eng_preds==valid_data$label)

bin_rad_preds <- case_when(predict(bin_rad_mod, 
                                valid_data, 
                                type = 'response') > 0.5 ~ 1, T ~ 0)
bin_rad_acc <- mean(bin_rad_preds==valid_data$label)

bin_full_preds <- case_when(predict(bin_full_mod, 
                                valid_data, 
                                type = 'response') > 0.5 ~ 1, T ~ 0)
bin_full_acc <- mean(bin_full_preds==valid_data$label)

# Printing binomial logistic regression accuracies
#c(bin_eng_acc, bin_rad_acc, bin_full_acc)
```

### Multinomial Logistic Regression

Multinomial logistic regression takes the same principles as binomial logistic regression but extends it to the multinomial case, where we have multiple categories. The same assumptions with logistic regression apply and are reasonable. This does treat the data as nominal categorical data rather than ordinal, which is debatable as you could perceive some ordering in the labels of no clouds, uncertain, clouds, and the EDA did show some trend in that.

```{r multinomial logistic, message = F, results='hide'}
# Training multinomial regression models
multi_eng_mod <- multinom(engineered_form, data = train_data)
multi_rad_mod <- multinom(radiance_form, data = train_data)
multi_full_mod <- multinom(full_form, data = train_data)

# Calculating predictions and accuracies for each model
multi_eng_preds <- predict(multi_eng_mod, valid_data, type = 'class')
multi_eng_acc <- mean(multi_eng_preds==valid_data$label)

multi_rad_preds <- predict(multi_rad_mod, valid_data, type = 'class')
multi_rad_acc <- mean(multi_rad_preds==valid_data$label)

multi_full_preds <- predict(multi_full_mod, valid_data, type = 'class')
multi_full_acc <- mean(multi_full_preds==valid_data$label)

# Printing multinomial regression accuracies
#c(multi_eng_acc, multi_rad_acc, multi_full_acc)
```

### Ridge Regression

Ridge regression takes the principles of linear regression (logistic regression in this case) and adds a penalization term to the training process as a method of regularization. Ridge essentially works by "shrinking" the parameters of the model closer to zero, introducing a bias into the model in an attempt to reduce the variance when generalizing to other data, taking advantage of the bias-variance tradeoff in minimizing MSE. This "shrinking" requires standardized data to apply it fairly across variables and otherwise operates in the same assumption framework as before with binomial logistic regression. We use the binary scaled data for this model.

Ridge regression has one hyper-parameter, $\lambda$, which is essentially how heavily we penalize the model. We chose this hyper-parameter through 10-fold cross validation on the training set and do so for each of the three models (sets of predictors), taking the optimal value and refitting the model using this value for future testing.

```{r ridge regression, message=FALSE}
# Setting up range of possible hyperparameter values
lambdas = 10^seq(2, -3, by = -.1)

# Engineered Model
# Cross validation to find optimal lambda hyperparameter value
ridge_eng_mod_cv <- cv.glmnet(as.matrix(train_data_binary_scaled[,c(4:6)]), 
                 train_data_binary_scaled$label, 
                 alpha = 0, lambda = lambdas)

# Running ridge regression using optimal hyperparameter value
ridge_eng_mod = glmnet(as.matrix(train_data_binary_scaled[,c(4:6)]), 
                   train_data_binary_scaled$label, 
                   lambda = ridge_eng_mod_cv$lambda.min, 
                   alpha = 0, family = 'binomial')

# Getting ridge regression predictions
ridge_eng_preds <- case_when(predict(ridge_eng_mod, 
                           as.matrix(valid_data_binary_scaled[,c(4:6)]),
                           s = ridge_eng_mod_cv$lambda.min, 
                           'response')  > 0.5 ~ 1, T ~ 0)
# Calculating ridge regression accuracies
ridge_eng_acc <- mean(ridge_eng_preds==valid_data_binary_scaled$label)



# Radiance Model
ridge_rad_mod_cv <- cv.glmnet(as.matrix(train_data_binary_scaled[,c(7:11)]),
                 train_data_binary_scaled$label, 
                 alpha = 0, lambda = lambdas)

ridge_rad_mod = glmnet(as.matrix(train_data_binary_scaled[,c(7:11)]), 
                   train_data_binary_scaled$label, 
                   lambda = ridge_rad_mod_cv$lambda.min, 
                   alpha = 0, family = 'binomial')

ridge_rad_preds <- case_when(predict(ridge_rad_mod, 
                           as.matrix(valid_data_binary_scaled[,c(7:11)]),
                           s = ridge_rad_mod_cv$lambda.min, 
                           'response')  > 0.5 ~ 1, T ~ 0)

ridge_rad_acc <- mean(ridge_rad_preds==valid_data_binary_scaled$label)




# Full Model
ridge_full_mod_cv <- cv.glmnet(as.matrix(train_data_binary_scaled[,c(4:11)]),
                 train_data_binary_scaled$label, 
                 alpha = 0, lambda = lambdas)

ridge_full_mod = glmnet(as.matrix(train_data_binary_scaled[,c(4:11)]), 
                   train_data_binary_scaled$label, 
                   lambda = ridge_full_mod_cv$lambda.min, 
                   alpha = 0, family = 'binomial')
ridge_full_preds <- case_when(predict(ridge_full_mod, 
                           as.matrix(valid_data_binary_scaled[,c(4:11)]),
                           s = ridge_full_mod_cv$lambda.min, 
                           'response')  > 0.5 ~ 1, T ~ 0)
ridge_full_acc <- mean(ridge_full_preds==valid_data_binary_scaled$label)

# Printing ridge regression accuracies
#c(ridge_eng_acc, ridge_rad_acc, ridge_full_acc)
```

### Random Forests

Random forests use a collection of decision trees to make predictions. The individual decision trees make decision points for classification based on the value of a particular predictor at a given break point and give a predicted class from there. The random forest takes the "majority vote" of a large number of decision trees to aggregate a larger sample and get better predictions. They also add randomness into the process by using a subset of data (bootstrapping) and variables for each tree, giving better generalization to the model.

This process has two hyper-parameters, the number of trees used and the number of randomly sampled variables used in each tree. Due to computational constraints, these hyper-parameters could not be rigorously obtained and so we use our best estimate of the optimal values. For the number of trees, we ran one iteration and saw that the out-of-bag error stabilized before 200 trees. We then use the value of 250 trees for the remainder of the models to give some room for variance between models. The other hyper-parameter, the number of variables used, has a default value for classification problems equal to the square-root of the total number of variables. This gives values of 1 for the engineered features model and 2 for the radiance and full models.

```{r Random Forests, message = F}
# Running random forest models using predetermined hyperparameters
rf_eng_mod <- randomForest(label ~ NDAI + SD + CORR, 
                           data = train_data, ntree = 250)

rf_rad_mod <- randomForest(label ~ DF + CF + BF + AF + AN, 
                           data = train_data, ntree = 250)

rf_full_mod <- randomForest(label ~ NDAI + SD + CORR + 
                             DF + CF + BF + AF + AN, 
                           data = train_data, ntree = 250)

# Getting random forest predictions
rf_eng_preds <- as.vector(predict(rf_eng_mod, valid_data[,4:6]))
rf_rad_preds <- as.vector(predict(rf_rad_mod, valid_data[,7:11]))
rf_full_preds <- as.vector(predict(rf_full_mod, valid_data[,4:11]))

# Calculating random forest accuracies
rf_eng_acc <- mean(rf_eng_preds==valid_data$label)
rf_rad_acc <- mean(rf_rad_preds==valid_data$label)
rf_full_acc <- mean(rf_full_preds==valid_data$label)

# Printing random forest accuracies
#c(rf_eng_acc, rf_rad_acc, rf_full_acc)
```

### Accuracy Table

Below we have all the accuracy rates for all twelve models. For the binary models, ridge regression performs drastically better than binomial logistic regression and for the three-class models, random forest performs decently better than multinomial logistic regression. We see a similar trend across all where the full model has the most accurate prediction, which is not surprising as we have more data to predict on. For the binary models, the engineered and radiance models perform similarly, and not at all far behind the full model. The multi-class models is where we see differentiation between models, with mutlinomial performing better with the engineered features and random forest with the radiances. If we wanted a binary model, ridge regression is the optimal model, and has far less computing demands than all the others. However, this will not distinguish between the uncertain and the no clouds observations, and is also dependent on how you treat the uncertain labels. Therefore we conclude random forest will yield the most accurate predictions and will take it as our final model.

```{r accuracy table}
# Creating a data table with all model accuracies
acc <- data.frame(
  Binomial = c(bin_eng_acc, bin_rad_acc, bin_full_acc),
  Multinomial = c(multi_eng_acc, multi_rad_acc, multi_full_acc),
  Ridge = c(ridge_eng_acc, ridge_rad_acc, ridge_full_acc),
  `Random Forest` = c(rf_eng_acc, rf_rad_acc, rf_full_acc),
  row.names = c('Engineered','Radiance','Full'))

# Printing rounded accuracy table
acc %>% round(3) %>% knitr::kable()
```

### Final Model

```{r final model}
# Running the random forest model on all train and validation data
final_model <- randomForest(label ~ NDAI + SD + CORR + 
                             DF + CF + BF + AF + AN, 
                           data = rbind(train_data,valid_data), 
                           ntree = 250)

# Getting final error using final model on test data
test_preds <- as.vector(predict(final_model, test_data[,4:11]))
final_acc <- mean(test_preds==test_data$label)

print(paste('Final Model Accuracy:',round(final_acc, 3)))
```

The final model does a good job of accurately predicting among the three class labels, having over 80% success. However, we still face the issue of what to do with the uncertain labels. Ideally we would be able to train using the full dataset but get a model that has only predicts clouds or no clouds for each pixel. This could be done with a two stage model, first using only data without an uncertain label, use that model to predict labels for our uncertain data, and then take that new full data to train a binary full model.

In terms of assessing future Terra sattelite images, we would expect our model to get around 80% of the pixels correctly labeled, as our testing data for the accuracy value above was not used at all in training and so we expect it to be representative of a generalizable error.
